#!/usr/bin/env python3
"""
Compute JJA ERA5 composites for MCS composite times at pressure levels
for specific climatological periods and save as a multidimensional netCDF file
with an extra weather type dimension. Includes WT0 (all events).

Includes calculation and storage of the corresponding mean MONTHLY-HOURLY
climatology for each composite group, allowing for later anomaly calculation.
Climatology files are expected to contain May-September data for specific periods,
but composites are calculated ONLY for JJA (June, July, August).

Reads a composite CSV file containing MCS event times and weather types ('wt').
Events are filtered for JJA AND for the selected period (e.g., 2000-2009),
then grouped by weather type, month, and time offset. Corresponding ERA5 data
and pre-calculated climatology data are read.

Output Composite File Contains:
- <var>_mean: Mean of the raw variable for the events in the group.
- <var>_clim_mean: Mean of the corresponding monthly-hourly climatology fields
                   for the specific event times in the group.
- event_count: Number of events contributing at each grid cell.

Dimensions: (weather_type, month, time_diff, level, latitude, longitude)
where 'month' will be [6, 7, 8].

Usage:
    python composite_pressure_levels.py --era5_dir /path/to/era5/pressure_levels/ \\
         --clim_base_dir ./climatology_output_custom/ \\
         --period evaluation \\
         --comp_csv_base ./csv/composite_ --region southern_alps --levels 250,500,850 \\
         --output_dir ./composite_output_plev_custom/ [--ncores 32] [--serial] [--debug]

Author: David Kneidinger (updated by Gemini)
Date: 2025-05-07
"""

import os
import sys
import argparse
from datetime import timedelta
import pandas as pd
import numpy as np
import xarray as xr
from multiprocessing import Pool
import metpy.calc as mpcalc
from metpy.units import units
import warnings
import logging
from typing import List, Dict, Tuple, Any, Optional, Set
from pathlib import Path

# --- Configuration ---
# Logging setup happens in main() based on --debug flag

DOMAIN_LAT_MIN, DOMAIN_LAT_MAX = 20, 55
DOMAIN_LON_MIN, DOMAIN_LON_MAX = -20, 40

# Base ERA5 variables needed for calculation.
VAR_LIST = ['z', 't', 'q', 'u', 'v', 'w']
DERIVED_VARS = ['theta_e'] # Variables calculated post-compositing
ALL_VARS_TO_SAVE = VAR_LIST + DERIVED_VARS # Variables to appear in output file

# Definitions for climatology periods to match climatologies_py_custom_periods.py
PERIODS = {
    "historical": {"start": 1996, "end": 2005, "name_in_file": "historical"},
    "evaluation": {"start": 2000, "end": 2009, "name_in_file": "evaluation"}
}
# Months to process and include in the output composite
TARGET_MONTHS = list(range(6, 9)) # JJA (June, July, August)
# Months expected in the climatology input files (generated by the other script)
CLIMATOLOGY_MONTHS = list(range(5, 10)) # May to September


# --- Helper Functions ---
def get_era5_file(era5_dir: Path, year: int, month: int) -> Path:
    """Construct the ERA5 monthly filename (e.g., "2005-08_NA.nc")."""
    fname = f"{year}-{month:02d}_NA.nc"
    return era5_dir / fname

def reorder_lat(ds: xr.Dataset) -> xr.Dataset:
    """Ensure latitude is in ascending order."""
    lat_coord_name = None
    if 'latitude' in ds.coords: lat_coord_name = 'latitude'
    elif 'lat' in ds.coords: lat_coord_name = 'lat'
    if lat_coord_name and ds[lat_coord_name].values.size > 1 and ds[lat_coord_name].values[0] > ds[lat_coord_name].values[-1]:
        logging.debug(f"Reordering {lat_coord_name} to ascending.")
        sorted_lat_values = np.sort(ds[lat_coord_name].values)
        ds = ds.reindex({lat_coord_name: sorted_lat_values})
    return ds

def fix_lat_lon_names(ds: xr.Dataset) -> xr.Dataset:
    """Ensure standard 'latitude' and 'longitude' coordinate names."""
    rename_dict = {}
    if 'lat' in ds.coords and 'latitude' not in ds.coords: rename_dict['lat'] = 'latitude'
    if 'lon' in ds.coords and 'longitude' not in ds.coords: rename_dict['lon'] = 'longitude'
    if rename_dict: ds = ds.rename(rename_dict)
    return ds

def create_offset_cols(df: pd.DataFrame) -> Dict[int, str]:
    """Automatically extract offset column names from the DataFrame."""
    offset_cols = {}
    found_base = False
    time_cols_in_csv = [c for c in df.columns if 'time' in c.lower()]
    for col in time_cols_in_csv:
        if col.startswith("time_minus"):
            try: offset_cols[-int(col.replace("time_minus", "").replace("h", ""))] = col
            except ValueError: logging.warning(f"Could not parse offset: {col}")
        elif col.startswith("time_plus"):
            try: offset_cols[int(col.replace("time_plus", "").replace("h", ""))] = col
            except ValueError: logging.warning(f"Could not parse offset: {col}")
        elif col == "time_0h":
            offset_cols[0] = col
            found_base = True
    if not found_base and 0 not in offset_cols:
        logging.warning("Base time column 'time_0h' not found.")
        if time_cols_in_csv:
            offset_cols[0] = time_cols_in_csv[0]
            logging.info(f"Using '{offset_cols[0]}' as base time (offset 0).")
        else: raise ValueError("No suitable time column found in CSV.")
    return offset_cols

# --- Core Processing Function ---
def process_month_level_events(task: Tuple) -> Dict[str, Any]:
    """
    Process one (year, month, times_pd, era5_dir, levels, clim_ds) task.
    Calculates sums and counts for base variables only. Theta-e is derived later.
    Returns a dictionary: {"composite": comp_data}
    """
    # Unpack task arguments
    year, month, times_pd, era5_dir, levels, clim_ds_input = task
    task_label = f"Y{year}-M{month:02d}"
    logging.debug(f"--- Starting Task: {task_label} ---")
    file_path = get_era5_file(era5_dir, year, month)

    # Initialize return structures for composite data (only base vars + count/coords)
    comp_data = {}
    for lev_val in levels:
        level_key = str(lev_val)
        comp_data[level_key] = {f"{var}_sum": None for var in VAR_LIST} # Only base vars
        comp_data[level_key].update({f"{var}_clim_sum": None for var in VAR_LIST}) # Only base vars
        comp_data[level_key].update({'count': None, 'lat': None, 'lon': None})

    # Check if the ERA5 data file exists
    if not file_path.exists():
        logging.warning(f"Task {task_label}: ERA5 file {file_path} not found. Skipping.")
        return {"composite": comp_data} # Return empty structure

    try:
        # --- Climatology Handling ---
        clim_month_ds = None
        if clim_ds_input is not None:
            # Validate climatology dataset coordinates
            if not {'month', 'hour', 'level'}.issubset(clim_ds_input.coords):
                logging.error(f"Task {task_label}: Climatology dataset missing required coordinates.")
                return {"composite": comp_data}
            try:
                # Select the relevant month from the climatology dataset and load into memory
                logging.debug(f"Task {task_label}: Selecting month {month} from climatology.")
                # Ensure the month is one of the JJA months for processing
                if month in TARGET_MONTHS: # Only select if it's a JJA month
                    clim_month_ds = clim_ds_input.sel(month=month).load()
                    logging.debug(f"Task {task_label}: clim_month_ds dims: {clim_month_ds.dims if clim_month_ds else 'None'}")
                else:
                    logging.debug(f"Task {task_label}: Month {month} is not a JJA month. Clim sum will be None.")
                    clim_month_ds = None # Ensure it's None if not a target month
            except KeyError:
                logging.debug(f"Task {task_label}: Month {month} not found in loaded climatology (expected May-Sep). Clim sum will be None.")
            except Exception as e:
                logging.error(f"Task {task_label}: Could not select month {month} from climatology: {e}")
        else:
            logging.warning(f"Task {task_label}: Climatology dataset (clim_ds_input) is None. Clim sums will be None.")

        # --- Load Raw ERA5 Data ---
        logging.debug(f"Task {task_label}: Loading raw data from {file_path}")
        with xr.open_dataset(file_path, chunks={'time': 'auto'}, cache=False) as ds:
            # Preprocess dataset: fix names, select domain, reorder latitude
            ds = fix_lat_lon_names(ds)
            ds = reorder_lat(ds) # Now reorder to ascending
            ds = ds.sel(latitude=slice(DOMAIN_LAT_MIN, DOMAIN_LAT_MAX),
                        longitude=slice(DOMAIN_LON_MIN, DOMAIN_LON_MAX))
            

            # Select data corresponding to event times
            logging.debug(f"Task {task_label}: Reindexing raw data to event times ({len(times_pd)} events)...")
            ds_sel_raw = ds.reindex(time=times_pd, method='nearest', tolerance=pd.Timedelta('1H'))
            logging.debug(f"Task {task_label}: Raw data time size after reindex: {ds_sel_raw.time.size}")
            ds_sel_raw = ds_sel_raw.dropna(dim="time", how="all") # Remove times where all data is NaN
            logging.debug(f"Task {task_label}: Raw data time size after dropna: {ds_sel_raw.time.size}")

            # Check if any valid data remains after selection
            if ds_sel_raw.time.size == 0:
                 logging.warning(f"Task {task_label}: No valid raw data found for specified times in {file_path}.")
                 return {"composite": comp_data}

            # Extract actual times and corresponding hours for climatology matching
            actual_event_times = pd.to_datetime(ds_sel_raw['time'].values)
            event_hours = actual_event_times.hour.to_numpy()
            logging.debug(f"Task {task_label}: Actual event hours selected: {event_hours}")

            # --- Process Each Level ---
            for lev_val in levels:
                level_key = str(lev_val)
                logging.debug(f"Task {task_label}: Processing Level {lev_val}hPa...")
                clim_level_month_aligned = None
                ds_level_raw_data = None # Initialize

                try:
                    # Select the specific pressure level from the raw data
                    ds_level_raw_data = ds_sel_raw.sel(level=lev_val, method="nearest")

                    # Check if data exists for this level after selection
                    if ds_level_raw_data.time.size == 0:  # or ds_level_raw_data.isnull().all():  TODO:something weird here
                        logging.warning(f"Task {task_label}: No valid raw data for level {lev_val} after level selection.")
                        comp_data[level_key]['count'] = None # Ensure count is None if skipping level
                        continue # Skip to the next level

                    # Align climatology grid if climatology data is available
                    if clim_month_ds is not None: # clim_month_ds is already for the correct month (JJA)
                        logging.debug(f"Task {task_label} L{lev_val}: Checking grid alignment...")
                        clim_level_month_ds = clim_month_ds.sel(level=lev_val, method="nearest")
                        try:
                            # Check for exact grid match
                            xr.align(ds_level_raw_data.isel(time=0), clim_level_month_ds, join="exact", copy=False)
                            clim_level_month_aligned = clim_level_month_ds # Grids match
                            logging.debug(f"Task {task_label} L{lev_val}: Grids match exactly.")
                        except ValueError as e_align:
                            # Log fatal error if grids don't match; skip climatology calculation
                            logging.error(f"Task {task_label} L{lev_val}: FATAL: Grid mismatch between raw data and climatology. Cannot proceed with clim sum.")
                            logging.error(f"Task {task_label} L{lev_val}: Alignment error details: {e_align}")
                            clim_level_month_aligned = None # Ensure we don't use it
                except Exception as e:
                    logging.warning(f"Task {task_label}: Level {lev_val} not found or error selecting level: {e}")
                    comp_data[level_key]['count'] = None # Ensure count is None if skipping level
                    continue # Skip this level

                # --- Composite Calculation ---
                logging.debug(f"Task {task_label} L{lev_val}: Calculating sums and counts...")
                base_vars_present = [v for v in VAR_LIST if v in ds_level_raw_data]
                if not base_vars_present:
                    logging.warning(f"Task {task_label} L{lev_val}: No base variables found. Cannot calculate count.")
                    comp_data[level_key]['count'] = None # Ensure count is None
                    continue # Skip level

                # Calculate count based on non-null values in any base variable
                count_arr = ds_level_raw_data[base_vars_present].to_array(dim="variable").notnull().any(dim="variable").sum(dim='time').compute()
                comp_data[level_key]['count'] = count_arr
                comp_data[level_key]['lat'] = ds_level_raw_data['latitude'].compute().values
                comp_data[level_key]['lon'] = ds_level_raw_data['longitude'].compute().values
                logging.debug(f"Task {task_label} L{lev_val}: count_arr shape: {count_arr.shape}, max: {np.max(count_arr) if count_arr.size>0 else 'N/A'}, NaNs: {np.isnan(count_arr).sum()}")

                # --- Process each base variable ---
                for var in VAR_LIST:
                    var_sum_arr = None
                    clim_sum_arr = None
                    if var in ds_level_raw_data:
                        raw_var_data = ds_level_raw_data[var] # Keep as DataArray for now
                        var_sum_arr = raw_var_data.sum(dim='time', skipna=True).compute()
                        comp_data[level_key][f"{var}_sum"] = var_sum_arr

                        # Calculate climatology sum if aligned and variable exists
                        if clim_level_month_aligned is not None and var in clim_level_month_aligned:
                            logging.debug(f"Task {task_label} L{lev_val} V={var}: Calculating climatology sum...")
                            try:
                                clim_data_hourly = clim_level_month_aligned[var].sel(hour=event_hours).compute().values
                                logging.debug(f"  Selected clim_data_hourly shape: {clim_data_hourly.shape}")
                                clim_sum_arr = np.nansum(clim_data_hourly, axis=0)
                                comp_data[level_key][f"{var}_clim_sum"] = clim_sum_arr
                            except Exception as e_clim_sel:
                                logging.warning(f"Task {task_label} L{lev_val} V={var}: Could not select/sum hourly clim: {e_clim_sel}")
                                comp_data[level_key][f"{var}_clim_sum"] = None
                        else:
                            logging.debug(f"Task {task_label} L{lev_val} V={var}: Skipping climatology sum (clim not aligned or var missing).")
                            comp_data[level_key][f"{var}_clim_sum"] = None
                    else: # Variable not in raw data
                        logging.warning(f"Task {task_label} L{lev_val}: Variable {var} not found in raw data.")
                        shape_sum = (len(comp_data[level_key]['lat']), len(comp_data[level_key]['lon'])) if comp_data[level_key]['lat'] is not None else None
                        if shape_sum:
                            comp_data[level_key][f"{var}_sum"] = np.full(shape_sum, np.nan)
                            comp_data[level_key][f"{var}_clim_sum"] = np.full(shape_sum, np.nan)
                        else: # Cannot determine shape if lat/lon are missing
                            comp_data[level_key][f"{var}_sum"] = None
                            comp_data[level_key][f"{var}_clim_sum"] = None


                    logging.debug(f"Task {task_label} L{lev_val} V={var}: Sum shape: {var_sum_arr.shape if var_sum_arr is not None else 'None'}, max: {np.nanmax(var_sum_arr) if var_sum_arr is not None and var_sum_arr.size>0 else 'N/A'}, NaNs: {np.isnan(var_sum_arr).sum() if var_sum_arr is not None else 'N/A'}")
                    logging.debug(f"Task {task_label} L{lev_val} V={var}: ClimSum shape: {clim_sum_arr.shape if clim_sum_arr is not None else 'None'}, max: {np.nanmax(clim_sum_arr) if clim_sum_arr is not None and clim_sum_arr.size > 0 else 'N/A'}, NaNs: {np.isnan(clim_sum_arr).sum() if clim_sum_arr is not None else 'N/A'}")

                # Theta-e sums will be calculated later from mean T/Q
                comp_data[level_key]["theta_e_sum"] = None
                comp_data[level_key]["theta_e_clim_sum"] = None

    except Exception as e:
        logging.error(f"Task {task_label}: General error processing task: {e}", exc_info=True)
        return {"composite": comp_data} # Return potentially partial data

    # Add final check before returning
    logging.debug(f"--- Finishing Task: {task_label} ---")
    for lev_val in levels:
        level_key = str(lev_val)
        if level_key in comp_data and comp_data[level_key].get('count') is not None:
             count_final = comp_data[level_key].get('count')
             logging.debug(f"  Final state L{lev_val}: Count max = {np.max(count_final) if count_final.size > 0 else 'N/A'}")
             for var in VAR_LIST:
                 sum_is_none = comp_data[level_key].get(f'{var}_sum') is None
                 clim_sum_is_none = comp_data[level_key].get(f'{var}_clim_sum') is None
                 logging.debug(f"    Final state L{lev_val} V={var}: sum is None? {sum_is_none}, clim_sum is None? {clim_sum_is_none}")
        else:
            logging.debug(f"  Final state L{lev_val}: No data or count is None.")


    return {"composite": comp_data}

# --- Combination Functions ---
def combine_tasks_results(task_results: List[Dict], levels: List[int]) -> Dict[str, Any]:
    """Combine task results for composites."""
    logging.debug(f"--- Combining results for {len(task_results)} tasks ---")
    # Initialize the overall structure to store combined results
    overall = {}
    for lev_val in levels:
        key = str(lev_val)
        # Initialize only for base variables + count/coords
        overall[key] = {f"{var}_sum": None for var in VAR_LIST}
        overall[key].update({f"{var}_clim_sum": None for var in VAR_LIST})
        overall[key].update({'count': None, 'lat': None, 'lon': None})

    valid_task_count = 0
    # Iterate through results from each task
    for i, result_dict in enumerate(task_results):
        # *** Access the 'composite' dictionary correctly ***
        result = result_dict.get("composite", {})
        if not result:
            logging.debug(f"  Combine task {i+1}: Skipping (no 'composite' key in result_dict)")
            continue

        task_had_valid_level = False
        # Iterate through each pressure level
        for lev_val in levels:
            key = str(lev_val)
            # Check if data exists for this level in the current task result
            res_level = result.get(key) # Get the dict for the current level
            if res_level is None or res_level.get('count') is None:
                # logging.debug(f"  Combine task {i+1} L{lev_val}: Skipping level (no data or no count for this level)")
                continue

            count_val = res_level.get('count')
            # Ensure count is a numpy array and not zero everywhere
            if isinstance(count_val, (int, float)): count_val = np.array([[count_val]]) # Should not happen
            if count_val.size == 0 or np.all(count_val == 0):
                # logging.debug(f"  Combine task {i+1} L{lev_val}: Skipping level (count is zero or empty)")
                continue

            task_had_valid_level = True
            logging.debug(f"  Combine task {i+1} L{lev_val}: Processing valid level. Count max: {np.max(count_val)}")

            # Initialize overall structure if this is the first valid result for this level
            if overall[key]['lat'] is None and res_level.get('lat') is not None:
                overall[key]['lat'] = res_level.get('lat')
                overall[key]['lon'] = res_level.get('lon')
                overall[key]['count'] = count_val.astype(np.int32)
                # Initialize sum arrays, handling potential None values from task
                for var in VAR_LIST: # Only base vars
                    overall[key][f"{var}_sum"] = res_level.get(f"{var}_sum") if res_level.get(f"{var}_sum") is not None else np.zeros_like(count_val, dtype=np.float64)
                    overall[key][f"{var}_clim_sum"] = res_level.get(f"{var}_clim_sum") if res_level.get(f"{var}_clim_sum") is not None else np.zeros_like(count_val, dtype=np.float64)
                logging.debug(f"    Initialized overall sums/counts for L{lev_val}.")
            elif overall[key]['lat'] is not None: # Accumulate subsequent results
                # Check for shape consistency before accumulating
                if overall[key]['count'] is not None and overall[key]['count'].shape == count_val.shape:
                    overall[key]['count'] += count_val.astype(np.int32)
                    # Accumulate sums for base variables
                    for var in VAR_LIST:
                        for sum_type in [f"{var}_sum", f"{var}_clim_sum"]:
                            current_sum = overall[key].get(sum_type)
                            new_sum = res_level.get(sum_type)
                            if new_sum is not None:
                                if current_sum is not None and current_sum.shape == new_sum.shape:
                                    overall[key][sum_type] += new_sum # Add new sum to existing
                                    logging.debug(f"      Accumulated {sum_type} L{lev_val} V={var}. New max: {np.nanmax(overall[key][sum_type]):.2f}")
                                elif current_sum is None: # Should ideally be initialized to zeros
                                    overall[key][sum_type] = new_sum.copy()
                                    logging.debug(f"      Initialized {sum_type} L{lev_val} V={var} from task {i+1}.")
                                else:
                                     # Log warning if shapes mismatch
                                     logging.warning(f"    Combine task {i+1} L{lev_val} V={var} Type={sum_type}: Shape mismatch. Overall: {current_sum.shape}, New: {new_sum.shape}. Skipping.")
                            # else: new_sum is None, do nothing
                else:
                    # Log warning if count shapes mismatch
                    logging.warning(f"  Combine task {i+1} L{lev_val}: Shape mismatch for count. Overall: {overall[key]['count'].shape if overall[key]['count'] is not None else 'None'}, New: {count_val.shape}. Skipping accumulation.")
        # Increment count of tasks that had at least one valid level
        if task_had_valid_level:
            valid_task_count += 1

    logging.debug(f"--- Finished Combining Results. Processed {valid_task_count}/{len(task_results)} valid tasks. ---")
    # Final check: fill with NaN if sums are still None after combining all tasks
    for lev_val in levels:
        key = str(lev_val)
        if overall[key].get('lat') is not None and overall[key].get('lon') is not None:
            final_shape = (len(overall[key]['lat']), len(overall[key]['lon']))
            for var in VAR_LIST: # Check only base vars
                if overall[key].get(f"{var}_sum") is None:
                    logging.debug(f"  Final combined {var}_sum L{lev_val} is None. Filling with NaN.")
                    overall[key][f"{var}_sum"] = np.full(final_shape, np.nan, dtype=np.float64)
                if overall[key].get(f"{var}_clim_sum") is None:
                    logging.debug(f"  Final combined {var}_clim_sum L{lev_val} is None. Filling with NaN.")
                    overall[key][f"{var}_clim_sum"] = np.full(final_shape, np.nan, dtype=np.float64)
            if overall[key].get('count') is None: # Should not happen if lat/lon are set
                 logging.debug(f"  Final combined count L{lev_val} is None. Filling with zeros.")
                 overall[key]['count'] = np.zeros(final_shape, dtype=np.int32)
        else:
             logging.warning(f"  No valid lat/lon found after combining tasks for L{lev_val}.")

    return overall


# --- NetCDF Saving Functions ---
def save_composites_to_netcdf(results_wt: Dict, weather_types: List, months: List[int],
                              time_offsets: List[int], levels: List[int],
                              lat: np.ndarray, lon: np.ndarray,
                              output_file: Path, period_details: Dict,
                              processed_years: Set[int], missing_years: List[int]):
    """
    Save composite means (raw and climatology) and counts to a NetCDF file.
    Calculates theta_e mean from T/Q means.
    Includes information about processed and missing years in attributes.
    """
    logging.info(f"--- Saving final composite NetCDF to: {output_file} ---")
    # Get dimensions sizes
    n_wt = len(weather_types); n_months = len(months); n_offsets = len(time_offsets)
    n_levels = len(levels); nlat = len(lat); nlon = len(lon)

    # Initialize arrays to store composite data
    comp_arrays_mean = {var: np.full((n_wt,n_months,n_offsets,n_levels,nlat,nlon), np.nan, dtype=np.float32) for var in ALL_VARS_TO_SAVE}
    comp_arrays_clim_mean = {var: np.full((n_wt,n_months,n_offsets,n_levels,nlat,nlon), np.nan, dtype=np.float32) for var in ALL_VARS_TO_SAVE}
    comp_arrays_count_grid = np.full((n_wt,n_months,n_offsets,n_levels,nlat,nlon), 0, dtype=np.int32)

    logging.debug("Populating final arrays for NetCDF...")
    # Iterate through dimensions to populate arrays
    for wi, wt in enumerate(weather_types):
        for mi, m_val in enumerate(months):
            for oi, off in enumerate(time_offsets):
                comp_month_data = results_wt.get(wt, {}).get(off, {}).get(m_val, None)
                if comp_month_data is None:
                    logging.debug(f"  WT={wt}, M={m_val}, Off={off}h: No data for this month/offset/wt group.")
                    continue
                for li, lev_val in enumerate(levels):
                    key = str(lev_val)
                    group_label = f"WT={wt}, M={m_val}, Off={off}h, L={lev_val}hPa"
                    comp_level_data = comp_month_data.get(key) # Use .get for safety
                    if comp_level_data is None:
                        logging.debug(f"  {group_label}: No combined data found for level.")
                        continue

                    count_arr = comp_level_data.get('count')
                    # Skip if count data is missing or all counts are zero
                    if count_arr is None or count_arr.size == 0 or np.all(count_arr == 0):
                        logging.debug(f"  {group_label}: Combined count is zero or None.")
                        continue

                    # Store grid counts
                    comp_arrays_count_grid[wi,mi,oi,li,:,:] = count_arr
                    logging.debug(f"  {group_label}: Count max = {np.max(count_arr)}")

                    # Calculate and store mean and climatology mean for BASE variables
                    for var in VAR_LIST:
                        for sum_type, mean_array_target in [(f"{var}_sum", comp_arrays_mean[var]),
                                                             (f"{var}_clim_sum", comp_arrays_clim_mean[var])]:
                            var_sum_val = comp_level_data.get(sum_type)
                            if var_sum_val is not None:
                                # Calculate mean, avoiding division by zero
                                with np.errstate(divide='ignore', invalid='ignore'):
                                    mean_val = var_sum_val / count_arr
                                # Store mean where count is greater than zero, otherwise NaN
                                mean_array_target[wi,mi,oi,li,:,:] = np.where(count_arr > 0, mean_val, np.nan)
                                if sum_type == f"{var}_sum":
                                    logging.debug(f"    {var}_mean calculated. Max: {np.nanmax(mean_val):.2f}, NaN count: {np.isnan(mean_val).sum()}/{mean_val.size}")
                                else:
                                    logging.debug(f"    {var}_clim_mean calculated. Max: {np.nanmax(mean_val):.2f}, NaN count: {np.isnan(mean_val).sum()}/{mean_val.size}")
                            else:
                                logging.debug(f"    {sum_type} was None for {group_label} V={var}.")

                    # --- Calculate Theta-e mean from T/Q means ---
                    logging.debug(f"  {group_label}: Calculating theta_e_mean...")
                    t_mean = comp_arrays_mean['t'][wi, mi, oi, li, :, :]
                    q_mean = comp_arrays_mean['q'][wi, mi, oi, li, :, :]
                    if not np.all(np.isnan(t_mean)) and not np.all(np.isnan(q_mean)):
                        try:
                            p_val_units = lev_val * units.hPa
                            t_mean_units = t_mean * units.kelvin
                            q_mean_units = q_mean * units('kg/kg')
                            valid_mask_mean = np.isfinite(t_mean_units) & np.isfinite(q_mean_units) & (t_mean_units > 0*units.kelvin) & (q_mean_units >= 0*units('kg/kg'))
                            if np.any(valid_mask_mean):
                                t_v = t_mean_units[valid_mask_mean]; q_v = q_mean_units[valid_mask_mean]
                                sat_mr = mpcalc.saturation_mixing_ratio(p_val_units, t_v)
                                q_v = np.minimum(q_v, 1.2 * sat_mr) # Cap q
                                dewpoint = mpcalc.dewpoint_from_specific_humidity(p_val_units, t_v, q_v)
                                theta_e_mean_val = mpcalc.equivalent_potential_temperature(p_val_units, t_v, dewpoint)
                                comp_arrays_mean['theta_e'][wi,mi,oi,li][valid_mask_mean] = theta_e_mean_val.magnitude
                                logging.debug(f"    theta_e_mean calculated. Max: {np.nanmax(theta_e_mean_val.magnitude):.2f}")
                            else: logging.debug(f"    Skipping theta_e_mean (no valid T/Q points).")
                        except Exception as e_th_mean: logging.warning(f"    Theta_e mean calculation error for {group_label}: {e_th_mean}")
                    else: logging.debug(f"    Skipping theta_e_mean (T or Q mean is all NaN).")

                    # --- Calculate Theta-e climatology mean from T/Q clim means ---
                    logging.debug(f"  {group_label}: Calculating theta_e_clim_mean...")
                    t_clim_mean = comp_arrays_clim_mean['t'][wi, mi, oi, li, :, :]
                    q_clim_mean = comp_arrays_clim_mean['q'][wi, mi, oi, li, :, :]
                    if not np.all(np.isnan(t_clim_mean)) and not np.all(np.isnan(q_clim_mean)):
                        try:
                            p_val_units = lev_val * units.hPa
                            t_clim_mean_units = t_clim_mean * units.kelvin
                            q_clim_mean_units = q_clim_mean * units('kg/kg')
                            valid_mask_clim_mean = np.isfinite(t_clim_mean_units) & np.isfinite(q_clim_mean_units) & (t_clim_mean_units > 0*units.kelvin) & (q_clim_mean_units >= 0*units('kg/kg'))
                            if np.any(valid_mask_clim_mean):
                                t_c_v = t_clim_mean_units[valid_mask_clim_mean]; q_c_v = q_clim_mean_units[valid_mask_clim_mean]
                                sat_mr_c = mpcalc.saturation_mixing_ratio(p_val_units, t_c_v)
                                q_c_v = np.minimum(q_c_v, 1.2 * sat_mr_c) # Cap q
                                dewpoint_c = mpcalc.dewpoint_from_specific_humidity(p_val_units, t_c_v, q_c_v)
                                theta_e_clim_mean_val = mpcalc.equivalent_potential_temperature(p_val_units, t_c_v, dewpoint_c)
                                comp_arrays_clim_mean['theta_e'][wi,mi,oi,li][valid_mask_clim_mean] = theta_e_clim_mean_val.magnitude
                                logging.debug(f"    theta_e_clim_mean calculated. Max: {np.nanmax(theta_e_clim_mean_val.magnitude):.2f}")
                            else: logging.debug(f"    Skipping theta_e_clim_mean (no valid T/Q points).")
                        except Exception as e_th_clim_mean: logging.warning(f"    Theta_e clim mean calculation error for {group_label}: {e_th_clim_mean}")
                    else: logging.debug(f"    Skipping theta_e_clim_mean (T or Q clim_mean is all NaN).")


    logging.debug("Creating xarray Dataset...")
    # Create dictionary of DataArrays for the final dataset
    ds_vars = {}
    for var in ALL_VARS_TO_SAVE:
        ds_vars[f"{var}_mean"] = xr.DataArray(comp_arrays_mean[var], dims=("weather_type","month","time_diff","level","latitude","longitude"), coords={"weather_type":weather_types, "month":months, "time_diff":time_offsets, "level":levels, "latitude":lat, "longitude":lon}, name=f"{var}_mean", attrs={"long_name":f"Mean raw {var}"})
        ds_vars[f"{var}_clim_mean"] = xr.DataArray(comp_arrays_clim_mean[var], dims=("weather_type","month","time_diff","level","latitude","longitude"), coords={"weather_type":weather_types, "month":months, "time_diff":time_offsets, "level":levels, "latitude":lat, "longitude":lon}, name=f"{var}_clim_mean", attrs={"long_name":f"Mean JJA climatology of {var}"})
        # Add units for theta_e if calculated
        if var == 'theta_e':
             ds_vars[f"{var}_mean"].attrs['units'] = 'K'
             ds_vars[f"{var}_clim_mean"].attrs['units'] = 'K'

    # Add the grid-cell count variable
    ds_vars["event_count"] = xr.DataArray(comp_arrays_count_grid, dims=("weather_type","month","time_diff","level","latitude","longitude"), coords={"weather_type":weather_types, "month":months, "time_diff":time_offsets, "level":levels, "latitude":lat, "longitude":lon}, name="event_count", attrs={"long_name":"Number of events at each grid cell"})

    # Create the final xarray Dataset
    ds = xr.Dataset(ds_vars)
    # Add global attributes to the dataset
    ds.attrs["description"] = (f"JJA ERA5 composites for pressure-level variables for MCS environments, "
                               f"stratified by weather type. Climatology based on JJA for period '{period_details['name_in_file']}' ({period_details['start']}-{period_details['end']}).")
    ds.attrs["history"] = f"Created by composite_pressure_levels.py on {pd.Timestamp.now(tz='UTC').strftime('%Y-%m-%d %H:%M:%S %Z')}"
    ds.attrs["source"] = "ERA5 pressure level data"
    ds.attrs["climatology_source_period_name"] = period_details['name_in_file']
    ds.attrs["climatology_source_period_years"] = f"{period_details['start']}-{period_details['end']}"
    ds.attrs["climatology_months_included"] = "May-September (used for clim), composites for June-August"
    # Add attributes for processed and missing years
    ds.attrs["data_period_years_processed"] = ", ".join(map(str, sorted(list(processed_years))))
    if missing_years:
        ds.attrs["data_period_years_missing_in_csv"] = ", ".join(map(str, sorted(missing_years)))
    else:
        ds.attrs["data_period_years_missing_in_csv"] = "None"


    # Define encoding for NetCDF saving (compression, fill value)
    encoding = {vname: {'zlib': True, 'complevel': 4, '_FillValue': np.float32(1e20)} for vname in ds.data_vars}
    # Ensure output directory exists
    output_file.parent.mkdir(parents=True, exist_ok=True)
    # Save the dataset to NetCDF
    ds.to_netcdf(output_file, encoding=encoding, mode='w')
    logging.info(f"Saved composite data to {output_file}")

# --- Main Script ---
def main():
    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(
        description="Compute JJA monthly ERA5 pressure level composites for MCS events, using period-specific climatology.")
    parser.add_argument("--era5_dir", type=Path, default='/data/reloclim/normal/INTERACT/ERA5/pressure_levels/',
                        help="Directory containing ERA5 monthly files (e.g., 2005-08_NA.nc)")
    parser.add_argument("--clim_base_dir", type=Path, default="/home/dkn/climatology/ERA5/",
                        help="Base directory where period-specific JJA climatology files are stored.")
    parser.add_argument("--period", type=str, default="evaluation", choices=PERIODS.keys(),
                        help=f"Climatology period to use: {list(PERIODS.keys())}. Default: evaluation.")
    parser.add_argument("--comp_csv_base", type=str, default='/nas/home/dkn/Desktop/MoCCA/composites/scripts/synoptic_composites/csv/composite_',
                        help="Base path for composite CSV files (e.g., './csv/composite_')")
    parser.add_argument("--levels", type=str, default="250,500,850",
                        help="Comma-separated pressure levels in hPa (e.g., 250,500,850)")
    parser.add_argument("--region", type=str, required=True,
                        help="Subregion name, used to find CSV file (e.g., southern_alps)")
    parser.add_argument("--output_dir", type=Path, default='/home/dkn/composites/ERA5/',
                        help="Directory to save output composite netCDF files")
    parser.add_argument("--ncores", type=int, default=32, help="Number of cores for parallel processing")
    parser.add_argument("--serial", action='store_true', help="Run in serial mode for debugging")
    parser.add_argument("--time_offsets", type=str, default="-12,0,12",
                        help="Comma-separated list of time offsets in hours (e.g., -12,-6,0,6,12)")
    parser.add_argument("--debug", action="store_true", help="Enable DEBUG level logging.")
    parser.add_argument("--noMCS", action="store_true", help="False: composite of initMCS; True: composite of noMCS; Default=False")
    args = parser.parse_args()

    # --- Setup Logging ---
    log_level = logging.DEBUG if args.debug else logging.INFO
    log_filename = "composite_pressure_level.log"
    # Configure logging to write to file and console
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, mode='w'), # Write to file, overwrite each run
            logging.StreamHandler(sys.stdout) # Also print to console
        ],
        force=True # Override any basicConfig called by libraries
    )
    logging.info("--- Starting Pressure Level Composite Script ---")
    logging.info(f"Run arguments: {args}")
    logging.info(f"Logging output to: {log_filename}")


    # Parse pressure levels from arguments
    levels = [int(l.strip()) for l in args.levels.split(',')]

    if args.noMCS:
        # Construct path to the composite CSV file
        comp_csv_file = Path(f"{args.comp_csv_base}{args.region}_nomcs.csv")
        if not comp_csv_file.exists():
            logging.error(f"Composite CSV file not found: {comp_csv_file}"); sys.exit(1)

        args_time_offsets = "0"
    else:
        # Construct path to the composite CSV file
        comp_csv_file = Path(f"{args.comp_csv_base}{args.region}_mcs.csv")
        if not comp_csv_file.exists():
            logging.error(f"Composite CSV file not found: {comp_csv_file}"); sys.exit(1)
        args_time_offsets = args.time_offsets

    # --- Load and Prepare Climatology Data ---
    # Get details for the selected period
    selected_period_details = PERIODS[args.period]
    clim_period_name_in_file = selected_period_details["name_in_file"]
    clim_start_year_period = selected_period_details["start"] # Renamed for clarity
    clim_end_year_period = selected_period_details["end"]   # Renamed for clarity

    # Build paths to required climatology files
    clim_files_to_load = []
    clim_base_var_dir = args.clim_base_dir
    for var in VAR_LIST:
        # Construct filename based on convention from climatology script
        # Assumes climatology files are May-Sep, as generated by the other script
        fname = f"era5_plev_{var}_clim_may_sep_{clim_period_name_in_file}_{clim_start_year_period}-{clim_end_year_period}.nc"
        fpath = clim_base_var_dir / fname # Climatology files are directly in clim_base_dir
        if not fpath.exists():
            logging.error(f"Required climatology file not found: {fpath}")
            logging.error("Please ensure climatologies for all VAR_LIST variables have been generated for May-Sep.")
            sys.exit(1)
        clim_files_to_load.append(fpath)

    # Load and merge climatology files
    clim_ds = None
    datasets_to_merge = []
    try:
        logging.info(f"Loading and merging {len(clim_files_to_load)} climatology files for period '{args.period}'...")
        for f in clim_files_to_load:
            logging.debug(f"  Opening clim file: {f}")
            datasets_to_merge.append(xr.open_dataset(f))

        # Merge datasets, overriding coordinates if necessary (use with caution)
        clim_ds = xr.merge(datasets_to_merge, compat='override')
        logging.info("Climatology files merged.")

        # Preprocess merged climatology dataset
        clim_ds = fix_lat_lon_names(clim_ds)
        clim_ds = clim_ds.sel(latitude=slice(DOMAIN_LAT_MIN, DOMAIN_LAT_MAX),
                              longitude=slice(DOMAIN_LON_MIN, DOMAIN_LON_MAX))
        clim_ds = reorder_lat(clim_ds)

        # Validate merged climatology dataset
        if not {'month', 'hour', 'level'}.issubset(clim_ds.dims):
            raise ValueError("Merged climatology must have 'month', 'hour', 'level' dimensions.")
        missing_vars = [v for v in VAR_LIST if v not in clim_ds]
        if missing_vars:
            raise ValueError(f"Variables {missing_vars} not found in merged climatology.")
        # Check if months in climatology match expected May-Sep
        clim_months_present = np.sort(clim_ds.month.values)
        if not np.array_equal(clim_months_present, CLIMATOLOGY_MONTHS):
             logging.warning(f"Merged climatology months {clim_months_present} do not match expected May-Sep {CLIMATOLOGY_MONTHS}.")
        logging.info(f"Climatology for PLEV vars (Period: {args.period}, Months: May-Sep) loaded and merged.")
    except Exception as e:
        logging.error(f"Failed to load/merge/process climatology files: {e}", exc_info=True)
        # Ensure datasets are closed even on error
        if clim_ds is not None: clim_ds.close()
        for ds_item in datasets_to_merge: ds_item.close()
        sys.exit(1)
    finally:
        # Close individual datasets after merging
        for ds_item in datasets_to_merge: ds_item.close()

    # --- Load and Filter Event Data ---
    if args.noMCS:
        base_col = "datetime"

    else:
        base_col = 'time_0h' # Reference time column in CSV
    try:
        # Load the CSV file, parsing the base time column as dates
        df_all = pd.read_csv(comp_csv_file, parse_dates=[base_col])
    except KeyError:
        logging.error(f"Base time column '{base_col}' not found in {comp_csv_file}.")
        if clim_ds is not None: clim_ds.close(); sys.exit(1)
    except Exception as e:
        logging.error(f"Error reading CSV {comp_csv_file}: {e}")
        if clim_ds is not None: clim_ds.close(); sys.exit(1)

    # Round base time to the nearest hour
    df_all[base_col] = df_all[base_col].dt.round("H")

    # --- Filter by Period and Months ---
    # Extract year and month from the base time column
    df_all['year'] = df_all[base_col].dt.year
    df_all['event_month_for_filter'] = df_all[base_col].dt.month

    # Filter rows based on the selected period's year range
    logging.info(f"Filtering events for period: {args.period} ({clim_start_year_period}-{clim_end_year_period})")
    df_period = df_all[(df_all['year'] >= clim_start_year_period) & (df_all['year'] <= clim_end_year_period)].copy()

    # Determine processed and missing years within the selected period
    target_years_set = set(range(clim_start_year_period, clim_end_year_period + 1))
    processed_years_set = set(df_period['year'].unique())
    missing_years_list = sorted(list(target_years_set - processed_years_set))

    if not processed_years_set:
         logging.warning(f"No events found in the CSV for the selected period {args.period} ({clim_start_year_period}-{clim_end_year_period}). Exiting.")
         if clim_ds is not None: clim_ds.close(); sys.exit(0)
    else:
        logging.info(f"Years processed from CSV within period: {sorted(list(processed_years_set))}")
        if missing_years_list:
            logging.warning(f"Years missing in CSV within period {args.period}: {missing_years_list}")
        else:
            logging.info(f"All years within period {args.period} present in CSV.")

    # Filter rows based on target months (JJA)
    logging.info(f"Filtering events for target months (JJA): {TARGET_MONTHS}")
    df_filtered = df_period[df_period['event_month_for_filter'].isin(TARGET_MONTHS)].copy()

    # Check if any events remain after filtering
    if df_filtered.empty:
        logging.info(f"No events found in JJA for region {args.region} within period {args.period}. Exiting.")
        if clim_ds is not None: clim_ds.close(); sys.exit(0)

    # --- Prepare for Processing ---
    try:
        # Get column names corresponding to time offsets
        offset_col_names = create_offset_cols(df_filtered)
        time_offsets = sorted([int(o) for o in args_time_offsets.split(',')])
       
        # Check if requested offsets exist in the CSV
        missing_offsets = [off for off in time_offsets if off not in offset_col_names]
        if missing_offsets:
             logging.error(f"Requested time offsets {missing_offsets} not found in CSV columns: {list(offset_col_names.values())}")
             if clim_ds is not None: clim_ds.close(); sys.exit(1)
    except ValueError as e:
        logging.error(f"Error determining time offset columns: {e}")
        if clim_ds is not None: clim_ds.close(); sys.exit(1)

    # Determine unique months present after filtering (should be subset of TARGET_MONTHS)
    months_to_process_for_output = sorted(df_filtered['event_month_for_filter'].unique())
    logging.info(f"Processing events for months: {months_to_process_for_output} for region {args.region}")

    # Check for weather type column and get unique types
    if 'wt' not in df_filtered.columns:
        logging.error(f"Weather type column 'wt' not found in {comp_csv_file}.")
        if clim_ds is not None: clim_ds.close(); sys.exit(1)
    weather_types = sorted(list(df_filtered['wt'].unique())) # Use df_filtered here
    # Ensure WT 0 (all events) is included
    if 0 not in weather_types: weather_types = [0] + weather_types
    logging.info(f"Processing weather types: {weather_types}")

    # --- Initialize Results Dictionary ---
    results_wt = {wt: {off: {} for off in time_offsets} for wt in weather_types}

    # --- Main Processing Loops ---
    logging.info("--- Starting Processing Loops ---")
    # Loop through each weather type
    for wt in weather_types:
        # Select data for the current weather type (or all data if WT=0)
        df_wt = df_filtered[df_filtered['wt'] == wt].copy() if wt != 0 else df_filtered.copy()
        logging.info(f"Processing WT {wt} ('{'All Events' if wt==0 else f'Type {wt}'}') - {len(df_wt)} events (JJA, Period: {args.period})")
        if df_wt.empty: continue # Skip if no events for this WT

        # Loop through each time offset
        for off in time_offsets:
            offset_col = offset_col_names[off]
            if offset_col not in df_wt.columns:
                 logging.warning(f"Offset column '{offset_col}' missing for WT {wt}, offset {off}h. Skipping.")
                 continue

            logging.info(f"  Processing Offset: {off}h (Column: {offset_col})")
            df_wt_off = df_wt.copy()
            # Ensure offset time column is datetime type, coerce errors
            if not pd.api.types.is_datetime64_any_dtype(df_wt_off[offset_col]):
                 df_wt_off[offset_col] = pd.to_datetime(df_wt_off[offset_col], errors='coerce')
            df_wt_off = df_wt_off.dropna(subset=[offset_col]) # Remove rows where time conversion failed

            # Group data by year and month for processing
            df_wt_off['year_for_grouping'] = df_wt_off[offset_col].dt.year # Use offset time for year grouping
            df_wt_off['month_for_grouping'] = df_wt_off['event_month_for_filter'] # Use original event month (JJA)
            groups = df_wt_off.groupby(['year_for_grouping', 'month_for_grouping'])

            # Create tasks for parallel processing
            tasks = []
            for (year_group_val, month_group_val), group_data in groups:
                t_list_pd = pd.DatetimeIndex(group_data[offset_col].tolist())
                if not t_list_pd.empty:
                    # Append task tuple: (year, month, times, data_dir, levels, climatology_dataset)
                    tasks.append((year_group_val, month_group_val, t_list_pd, args.era5_dir, levels, clim_ds))

            logging.info(f"    WT {wt}, Offset {off}h: Created {len(tasks)} tasks (year-month groups).")
            if not tasks: continue

            # Execute tasks (serially or in parallel)
            monthly_tasks_results = []
            if args.serial or args.ncores == 1:
                logging.info(f"    Running {len(tasks)} tasks serially...")
                for task_idx, task_data in enumerate(tasks):
                    logging.debug(f"      Serial task {task_idx+1}/{len(tasks)}: Year {task_data[0]}, Month {task_data[1]}")
                    monthly_tasks_results.append(process_month_level_events(task_data))
            else:
                logging.info(f"    Running {len(tasks)} tasks in parallel using {args.ncores} cores...")
                with Pool(processes=args.ncores) as pool:
                    monthly_tasks_results = pool.map(process_month_level_events, tasks)
            logging.info(f"    Finished processing tasks for WT {wt}, Offset {off}h.")

            # Group results by month
            tasks_by_month_comp = {}
            for task_data, res in zip(tasks, monthly_tasks_results):
                _, month_val, _, _, _, _ = task_data
                tasks_by_month_comp.setdefault(month_val, []).append(res)

            # Combine results for each month
            month_composites = {}
            logging.info(f"    Combining results for WT {wt}, Offset {off}h...")
            for m_val_output in months_to_process_for_output: # Should only be 6, 7, 8
                if m_val_output in tasks_by_month_comp:
                    # *** CORRECTED: Pass the list of full result dictionaries ***
                    month_composites[m_val_output] = combine_tasks_results(tasks_by_month_comp[m_val_output], levels)
                else:
                    # Initialize empty structure if no tasks were run for this month
                    month_composites[m_val_output] = {str(lvl): {f"{var}_sum": None, f"{var}_clim_sum": None, 'count': None, 'lat':None, 'lon':None} for lvl in levels for var in VAR_LIST} # Initialize only base vars
                    for lvl in levels: month_composites[m_val_output][str(lvl)]['count'] = None
            # Store combined results for this WT and offset
            results_wt[wt][off] = month_composites
            logging.info(f"    Finished combining for WT {wt}, Offset {off}h.")

    logging.info("--- Finished Processing Loops ---")

    # --- Post-processing and Saving ---
    logging.info("--- Post-processing and Saving Results ---")
    lat, lon = None, None
    try:
        # Find the first valid latitude/longitude coordinates from the results
        found_coords = False
        for wt_chk in weather_types:
            for off_chk in time_offsets:
                 for m_chk in months_to_process_for_output: # Use filtered months [6,7,8]
                      for lev_chk in levels:
                          sample_comp = results_wt.get(wt_chk,{}).get(off_chk,{}).get(m_chk,{}).get(str(lev_chk),None)
                          if sample_comp and sample_comp.get('lat') is not None and sample_comp.get('lon') is not None:
                              lat = sample_comp['lat']; lon = sample_comp['lon']
                              found_coords = True; break
                      if found_coords: break
                 if found_coords: break
            if found_coords: break
        # If no valid coordinates found in results, try getting from climatology dataset
        if not found_coords:
            logging.warning("Could not find valid lat/lon in any processed composite. Using clim_ds for coords.")
            if clim_ds is not None:
                lat = clim_ds['latitude'].values; lon = clim_ds['longitude'].values
                if lat is not None and lon is not None: found_coords = True
            if not found_coords: raise KeyError("Could not find valid lat/lon.")
    except KeyError as e:
        logging.error(f"Could not retrieve lat/lon: {e}. Cannot save.")
        if clim_ds is not None: clim_ds.close(); sys.exit(1)

    # Check if lat/lon were successfully retrieved
    if lat is None or lon is None:
        logging.error("Lat/lon info missing. Cannot save.");
        if clim_ds is not None: clim_ds.close(); sys.exit(1)

    # Define output filename including the period
    if args.noMCS:
        output_file_comp = args.output_dir / f"composite_plev_{args.region}_wt_clim_{args.period}_nomcs.nc"
    else:
        output_file_comp = args.output_dir / f"composite_plev_{args.region}_wt_clim_{args.period}.nc"
    # Save the final composite data to NetCDF
    save_composites_to_netcdf(results_wt, weather_types, months_to_process_for_output,
                              time_offsets, levels, lat, lon, output_file_comp,
                              selected_period_details,
                              processed_years=processed_years_set, # Pass processed years
                              missing_years=missing_years_list) # Pass missing years

    # Close the climatology dataset
    if clim_ds is not None: clim_ds.close()
    logging.info("--- Processing complete. ---")

# --- Entry Point ---
if __name__ == '__main__':
    # Suppress specific warnings if needed
    warnings.filterwarnings("ignore", message="Relative humidity >120%, ensure proper units.")
    warnings.filterwarnings("ignore", message="invalid value encountered in divide")
    warnings.filterwarnings("ignore", message="Mean of empty slice")
    warnings.filterwarnings("ignore", message=".*Slicing is producing a large chunk.*")
    warnings.filterwarnings("ignore", message=".*Converting non-nanosecond precision datetime values to nanosecond precision.*")
    # Execute the main function
    main()
