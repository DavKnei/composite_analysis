#!/usr/bin/env python3
"""
Compute JJA ERA5 composites for MCS composite times at pressure levels
for specific climatological periods and save as a multidimensional netCDF file
with an extra weather type dimension. Includes WT0 (all events).

Includes calculation and storage of the corresponding mean MONTHLY-HOURLY
climatology for each composite group, allowing for later anomaly calculation.
Theta_e is calculated for each timestep before summation.
Climatology files are expected to contain May-September data for specific periods,
but composites are calculated ONLY for JJA (June, July, August).

Reads a composite CSV file containing MCS event times and weather types ('wt').
Events are filtered for JJA AND for the selected period (e.g., 2000-2009),
then grouped by weather type, month, and time offset. Corresponding ERA5 data
and pre-calculated climatology data are read.

Output Composite File Contains:
- <var>_mean: Mean of the raw variable for the events in the group.
- <var>_clim_mean: Mean of the corresponding monthly-hourly climatology fields
                   for the specific event times in the group.
- event_count: Number of events contributing at each grid cell.

Dimensions: (weather_type, month, time_diff, level, latitude, longitude)
where 'month' will be [6, 7, 8].

Usage:
    python composite_pressure_levels.py --era5_dir /path/to/era5/pressure_levels/ \\
         --clim_base_dir ./climatology_output_custom/ \\
         --period evaluation \\
         --comp_csv_base ./csv/composite_ --region southern_alps --levels 250,500,850 \\
         --output_dir ./composite_output_plev_custom/ [--ncores 32] [--serial] [--debug]

Author: David Kneidinger
Date: 2025-05-07
Last Modified: 2025-06-03
"""

import os
import sys
import argparse
from datetime import timedelta
import pandas as pd
import numpy as np
import xarray as xr
import yaml
from multiprocessing import Pool
import metpy.calc as mpcalc
from metpy.units import units
import warnings
import logging
from typing import List, Dict, Tuple, Any, Optional, Set
from pathlib import Path

# --- Configuration ---
# Logging setup happens in main() based on --debug flag

# Define the EURO-CORDEX boundaries
DOMAIN_LAT_MIN, DOMAIN_LAT_MAX = 25, 65
DOMAIN_LON_MIN, DOMAIN_LON_MAX = -20, 43

# Base ERA5 variables needed from input files.
VAR_LIST = ["z", "t", "q", "u", "v", "w"]
# Variables calculated per timestep from VAR_LIST, then summed.
CALCULATED_VARS = ["theta_e"]
ALL_VARS_TO_SAVE = VAR_LIST + CALCULATED_VARS  # Variables to appear in output file

# Definitions for climatology periods to match climatologies_py_custom_periods.py
PERIODS = {"historical": {"start": 2001, "end": 2020, "name_in_file": "historical"}}

# Months to process and include in the output composite
TARGET_MONTHS = list(range(6, 9))  # JJA (June, July, August)
# Months expected in the climatology input files (generated by the other script)
CLIMATOLOGY_MONTHS = list(range(5, 10))  # May to September


# --- Helper Functions ---
def get_era5_file(era5_dir: Path, year: int, month: int) -> Path:
    """Construct the ERA5 monthly filename (e.g., "2005-08_NA.nc")."""
    fname = f"{year}-{month:02d}_NA.nc"
    return era5_dir / fname


def reorder_lat(ds: xr.Dataset) -> xr.Dataset:
    """Ensure latitude is in ascending order."""
    lat_coord_name = None
    if "latitude" in ds.coords:
        lat_coord_name = "latitude"
    elif "lat" in ds.coords:
        lat_coord_name = "lat"
    if (
        lat_coord_name
        and ds[lat_coord_name].values.size > 1
        and ds[lat_coord_name].values[0] > ds[lat_coord_name].values[-1]
    ):
        logging.debug(f"Reordering {lat_coord_name} to ascending.")
        sorted_lat_values = np.sort(ds[lat_coord_name].values)
        ds = ds.reindex({lat_coord_name: sorted_lat_values})
    return ds


def fix_lat_lon_names(ds: xr.Dataset) -> xr.Dataset:
    """Ensure standard 'latitude' and 'longitude' coordinate names."""
    rename_dict = {}
    if "lat" in ds.coords and "latitude" not in ds.coords:
        rename_dict["lat"] = "latitude"
    if "lon" in ds.coords and "longitude" not in ds.coords:
        rename_dict["lon"] = "longitude"
    if rename_dict:
        ds = ds.rename(rename_dict)
    return ds


def create_offset_cols(df: pd.DataFrame) -> Dict[int, str]:
    """Automatically extract offset column names from the DataFrame."""
    offset_cols = {}
    found_base = False
    time_cols_in_csv = [c for c in df.columns if "time" in c.lower()]
    for col in time_cols_in_csv:
        offset_val_str = (
            col.replace("time_minus", "")
            .replace("time_plus", "")
            .replace("h", "")
            .replace("time_", "")
        )
        if (
            col.startswith("time_minus")
            or col.startswith("time_plus")
            or col == "time_0h"
        ):
            offset_val = int(offset_val_str)  # Will raise ValueError if format is wrong
            if col.startswith("time_minus"):
                offset_cols[-offset_val] = col
            elif col.startswith("time_plus"):
                offset_cols[offset_val] = col
            elif col == "time_0h":
                offset_cols[0] = col
                found_base = True
        # Silently ignore other 'time' columns not matching the pattern
    if not found_base and 0 not in offset_cols:
        logging.warning("Base time column 'time_0h' not found.")
        if time_cols_in_csv:  # Try to find a fallback if 'time_0h' is missing
            # A simple heuristic: use the first parsable column as offset 0 if no other offset 0 is found
            # This part is a bit heuristic and might need refinement based on CSV conventions
            potential_base_candidates = [
                c for c in time_cols_in_csv if "minus" not in c and "plus" not in c
            ]
            if potential_base_candidates:
                offset_cols[0] = potential_base_candidates[
                    0
                ]  # Use the first such column
                logging.info(
                    f"Using '{offset_cols[0]}' as base time (offset 0) due to missing 'time_0h'."
                )
            else:
                raise ValueError(
                    "No suitable base time column (e.g., 'time_0h' or similar) found in CSV."
                )
        else:
            raise ValueError("No suitable time column found in CSV.")
    return offset_cols


def calculate_theta_e_from_xr(
    temp_da: xr.DataArray, spec_hum_da: xr.DataArray, pressure_val_hpa: float
) -> xr.DataArray:
    """
    Calculates equivalent potential temperature from xarray DataArrays of T and Q.
    Pressure is a scalar for the given level. T and Q can have multiple dimensions.
    Handles units and ensures calculations are NaN-robust.
    """
    p_units = pressure_val_hpa * units.hPa
    # Operate on .data (numpy arrays) for MetPy, then reconstruct DataArray
    t_data_np = temp_da.data
    q_data_np = spec_hum_da.data

    # Initialize result array with NaNs, preserving original shape
    theta_e_data_np = np.full_like(t_data_np, np.nan, dtype=np.float32)

    # MetPy expects units attached to numpy arrays
    t_units = t_data_np * units.kelvin
    q_units = q_data_np * units("kg/kg")

    # Create a mask for valid calculation points
    # Allow slightly negative q for robustness with model data, will be clipped.
    valid_mask = (
        np.isfinite(t_data_np)
        & np.isfinite(q_data_np)
        & (t_data_np > 0)
        & (q_data_np >= -1e-5)
    )

    # Clip q to be non-negative for calculations
    q_units_masked = np.where(valid_mask, q_units, np.nan)  # Apply mask before clipping
    q_units_masked[q_units_masked < 0 * units("kg/kg")] = 0 * units("kg/kg")

    if np.any(valid_mask):  # Proceed if there's any valid data
        t_v = t_units[valid_mask]
        q_v = q_units_masked[valid_mask]  # Use the masked and clipped q

        # Cap specific humidity at 1.2 * saturation mixing ratio to avoid issues
        # This is a common heuristic
        sat_mr = mpcalc.saturation_mixing_ratio(p_units, t_v)
        q_v = np.minimum(q_v, 1.2 * sat_mr)

        dewpoint = mpcalc.dewpoint_from_specific_humidity(p_units, t_v, q_v)
        theta_e_values = mpcalc.equivalent_potential_temperature(p_units, t_v, dewpoint)
        theta_e_data_np[valid_mask] = theta_e_values.magnitude

    return xr.DataArray(
        theta_e_data_np,
        coords=temp_da.coords,
        dims=temp_da.dims,
        name="theta_e",
        attrs={"units": "K"},
    )


# --- Core Processing Function ---
def process_month_level_events(task: Tuple) -> Dict[str, Any]:
    """
    Process one (year, month, times_pd, era5_dir, levels, clim_ds) task.
    Calculates sums and counts for base variables and theta_e.
    Returns a dictionary: {"composite": comp_data}
    """
    year, month, times_pd, era5_dir, levels, clim_ds_input = task
    task_label = f"Y{year}-M{month:02d}"
    logging.debug(f"--- Starting Task: {task_label} ---")
    file_path = get_era5_file(era5_dir, year, month)

    comp_data = {}
    for lev_val in levels:
        level_key = str(lev_val)
        comp_data[level_key] = {f"{var}_sum": None for var in ALL_VARS_TO_SAVE}
        comp_data[level_key].update(
            {f"{var}_clim_sum": None for var in ALL_VARS_TO_SAVE}
        )
        comp_data[level_key].update({"count": None, "lat": None, "lon": None})

    if not file_path.exists():
        logging.warning(
            f"Task {task_label}: ERA5 file {file_path} not found. Skipping."
        )
        return {"composite": comp_data}

    # --- Climatology Handling ---
    clim_month_ds = None
    if clim_ds_input is not None:
        if not {"month", "hour", "level"}.issubset(clim_ds_input.coords):
            # This is a critical setup error if climatology is expected.
            raise ValueError(
                f"Task {task_label}: Climatology dataset missing required coordinates (month, hour, level)."
            )

        if month in TARGET_MONTHS:  # Only select JJA months for processing
            # .sel will raise KeyError if month not found; this is desired to indicate an issue.
            clim_month_ds = clim_ds_input.sel(month=month).load()
            logging.debug(
                f"Task {task_label}: clim_month_ds dims: {clim_month_ds.dims if clim_month_ds else 'None'}"
            )
        else:
            logging.debug(
                f"Task {task_label}: Month {month} is not a JJA month. Clim sum will be None for this task's month."
            )
            # clim_month_ds remains None
    else:
        logging.warning(
            f"Task {task_label}: Climatology dataset (clim_ds_input) is None. Clim sums will be None."
        )

    # --- Load Raw ERA5 Data ---
    logging.debug(f"Task {task_label}: Loading raw data from {file_path}")
    with xr.open_dataset(file_path, chunks={"time": "auto"}, cache=False) as ds:
        ds = fix_lat_lon_names(ds)
        ds = reorder_lat(ds)
        ds = ds.sel(
            latitude=slice(DOMAIN_LAT_MIN, DOMAIN_LAT_MAX),
            longitude=slice(DOMAIN_LON_MIN, DOMAIN_LON_MAX),
        )

        logging.debug(
            f"Task {task_label}: Reindexing raw data to event times ({len(times_pd)} events)..."
        )
        # method='nearest' and tolerance are crucial.
        # If a time is not found within tolerance, it results in NaNs for that time.
        ds_sel_raw = ds.reindex(
            time=times_pd, method="nearest", tolerance=pd.Timedelta("1H")
        )
        logging.debug(
            f"Task {task_label}: Raw data time size after reindex: {ds_sel_raw.time.size}"
        )

        # Remove times where all data variables are NaN (e.g. due to reindex failure for those times)
        ds_sel_raw = ds_sel_raw.dropna(dim="time", how="all")
        logging.debug(
            f"Task {task_label}: Raw data time size after dropna: {ds_sel_raw.time.size}"
        )

        if ds_sel_raw.time.size == 0:
            logging.warning(
                f"Task {task_label}: No valid raw data found for specified times in {file_path}."
            )
            return {"composite": comp_data}

        actual_event_times = pd.to_datetime(ds_sel_raw["time"].values)
        event_hours = (
            actual_event_times.hour.to_numpy()
        )  # Used for selecting climatology hours
        logging.debug(f"Task {task_label}: Actual event hours selected: {event_hours}")

        # --- Process Each Level ---
        for lev_val in levels:
            level_key = str(lev_val)
            logging.debug(f"Task {task_label}: Processing Level {lev_val}hPa...")
            clim_level_month_aligned = None

            # .sel for level will raise KeyError if level not found, method='nearest' avoids this.
            ds_level_raw_data = ds_sel_raw.sel(level=lev_val, method="nearest")

            if ds_level_raw_data.time.size == 0:
                logging.warning(
                    f"Task {task_label}: No valid raw data for level {lev_val} after level selection or all data was NaN for this level."
                )
                comp_data[level_key]["count"] = None  # Ensure count is None if skipping
                continue

            if clim_month_ds is not None:
                clim_level_month_ds = clim_month_ds.sel(level=lev_val, method="nearest")
                # Check for exact grid match. This will raise ValueError if grids don't match.
                xr.align(
                    ds_level_raw_data.isel(time=0, drop=True),
                    clim_level_month_ds.isel(hour=0, drop=True),
                    join="exact",
                    copy=False,
                )
                clim_level_month_aligned = clim_level_month_ds
                logging.debug(f"Task {task_label} L{lev_val}: Grids match exactly.")

            # --- Composite Calculation ---
            logging.debug(
                f"Task {task_label} L{lev_val}: Calculating sums and counts..."
            )
            base_vars_present_in_raw = [v for v in VAR_LIST if v in ds_level_raw_data]
            if (
                not base_vars_present_in_raw
            ):  # Should not happen if file structure is correct
                logging.warning(
                    f"Task {task_label} L{lev_val}: No base variables found in raw data. Cannot calculate count or sums."
                )
                comp_data[level_key]["count"] = None
                continue

            # Calculate count based on non-null values in any base variable present
            count_arr = (
                ds_level_raw_data[base_vars_present_in_raw]
                .to_array(dim="variable_for_count")
                .notnull()
                .any(dim="variable_for_count")
                .sum(dim="time")
                .compute()
            )
            comp_data[level_key]["count"] = count_arr
            comp_data[level_key]["lat"] = ds_level_raw_data["latitude"].compute().values
            comp_data[level_key]["lon"] = (
                ds_level_raw_data["longitude"].compute().values
            )
            logging.debug(
                f"Task {task_label} L{lev_val}: count_arr shape: {count_arr.shape}, max: {np.max(count_arr) if count_arr.size>0 else 'N/A'}"
            )

            # --- Process each base variable from VAR_LIST ---
            for var in VAR_LIST:
                var_sum_arr = None
                clim_sum_arr = None
                if var in ds_level_raw_data:
                    raw_var_data = ds_level_raw_data[var]
                    var_sum_arr = raw_var_data.sum(dim="time", skipna=True).compute()
                    comp_data[level_key][f"{var}_sum"] = var_sum_arr

                    if (
                        clim_level_month_aligned is not None
                        and var in clim_level_month_aligned
                    ):
                        logging.debug(
                            f"Task {task_label} L{lev_val} V={var}: Calculating climatology sum..."
                        )
                        # .sel by event_hours (numpy array) will align correctly.
                        clim_data_for_events_np = (
                            clim_level_month_aligned[var]
                            .sel(hour=xr.DataArray(event_hours, dims="event_hours_dim"))
                            .compute()
                            .data
                        )
                        clim_sum_arr = np.nansum(
                            clim_data_for_events_np, axis=0
                        )  # Sum over the event hours dimension
                        comp_data[level_key][f"{var}_clim_sum"] = clim_sum_arr
                    else:
                        logging.debug(
                            f"Task {task_label} L{lev_val} V={var}: Skipping climatology sum (clim not aligned or var missing in clim)."
                        )
                        # Initialize with NaNs if shape is known
                        if comp_data[level_key]["lat"] is not None:
                            shape_sum = (
                                len(comp_data[level_key]["lat"]),
                                len(comp_data[level_key]["lon"]),
                            )
                            comp_data[level_key][f"{var}_clim_sum"] = np.full(
                                shape_sum, np.nan, dtype=np.float32
                            )
                else:
                    logging.warning(
                        f"Task {task_label} L{lev_val}: Variable {var} not found in raw data."
                    )
                    if comp_data[level_key]["lat"] is not None:
                        shape_sum = (
                            len(comp_data[level_key]["lat"]),
                            len(comp_data[level_key]["lon"]),
                        )
                        comp_data[level_key][f"{var}_sum"] = np.full(
                            shape_sum, np.nan, dtype=np.float32
                        )
                        comp_data[level_key][f"{var}_clim_sum"] = np.full(
                            shape_sum, np.nan, dtype=np.float32
                        )

                logging.debug(
                    f"Task {task_label} L{lev_val} V={var}: Sum shape: {var_sum_arr.shape if var_sum_arr is not None else 'None'}, NaNs: {np.isnan(var_sum_arr).sum() if var_sum_arr is not None else 'N/A'}"
                )
                logging.debug(
                    f"Task {task_label} L{lev_val} V={var}: ClimSum shape: {clim_sum_arr.shape if clim_sum_arr is not None else 'None'}, NaNs: {np.isnan(clim_sum_arr).sum() if clim_sum_arr is not None else 'N/A'}"
                )

            # --- Process theta_e (calculated per timestep) ---
            # RAW DATA THETA_E
            if (
                "t" in ds_level_raw_data
                and "q" in ds_level_raw_data
                and ds_level_raw_data.time.size > 0
            ):
                t_raw = ds_level_raw_data["t"].load()  # Load ensures data is in memory
                q_raw = ds_level_raw_data["q"].load()
                theta_e_instantaneous_raw = calculate_theta_e_from_xr(
                    t_raw, q_raw, float(lev_val)
                )
                comp_data[level_key]["theta_e_sum"] = theta_e_instantaneous_raw.sum(
                    dim="time", skipna=True
                ).compute()
                logging.debug(
                    f"Task {task_label} L{lev_val} V=theta_e: Raw Sum calculated. Shape: {comp_data[level_key]['theta_e_sum'].shape}"
                )
            else:
                logging.debug(
                    f"Task {task_label} L{lev_val}: Skipping theta_e_sum (raw T/Q missing or zero timesteps)."
                )
                if comp_data[level_key]["lat"] is not None:
                    shape_sum = (
                        len(comp_data[level_key]["lat"]),
                        len(comp_data[level_key]["lon"]),
                    )
                    comp_data[level_key]["theta_e_sum"] = np.full(
                        shape_sum, np.nan, dtype=np.float32
                    )

            # CLIMATOLOGY THETA_E
            if (
                clim_level_month_aligned is not None
                and "t" in clim_level_month_aligned
                and "q" in clim_level_month_aligned
                and clim_level_month_aligned.hour.size > 0
                and len(event_hours) > 0
            ):

                t_clim_hourly = clim_level_month_aligned["t"].load()
                q_clim_hourly = clim_level_month_aligned["q"].load()
                theta_e_clim_hourly_da = calculate_theta_e_from_xr(
                    t_clim_hourly, q_clim_hourly, float(lev_val)
                )

                selected_clim_theta_e = theta_e_clim_hourly_da.sel(
                    hour=xr.DataArray(event_hours, dims="event_hours_dim")
                )

                if selected_clim_theta_e.sizes.get("event_hours_dim", 0) > 0:
                    theta_e_clim_for_events_np = selected_clim_theta_e.compute().data
                    comp_data[level_key]["theta_e_clim_sum"] = np.nansum(
                        theta_e_clim_for_events_np, axis=0
                    )
                    logging.debug(
                        f"Task {task_label} L{lev_val} V=theta_e: Clim Sum calculated. Shape: {comp_data[level_key]['theta_e_clim_sum'].shape}"
                    )
                else:
                    logging.debug(
                        f"Task {task_label} L{lev_val}: Skipping theta_e_clim_sum (clim T/Q selection by event_hours yielded no data)."
                    )
                    if comp_data[level_key]["lat"] is not None:
                        shape_sum = (
                            len(comp_data[level_key]["lat"]),
                            len(comp_data[level_key]["lon"]),
                        )
                        comp_data[level_key]["theta_e_clim_sum"] = np.full(
                            shape_sum, np.nan, dtype=np.float32
                        )
            else:
                logging.debug(
                    f"Task {task_label} L{lev_val}: Skipping theta_e_clim_sum (clim T/Q missing, zero hours, or no event_hours)."
                )
                if comp_data[level_key]["lat"] is not None:
                    shape_sum = (
                        len(comp_data[level_key]["lat"]),
                        len(comp_data[level_key]["lon"]),
                    )
                    comp_data[level_key]["theta_e_clim_sum"] = np.full(
                        shape_sum, np.nan, dtype=np.float32
                    )

    # Add final check before returning
    logging.debug(f"--- Finishing Task: {task_label} ---")
    for lev_val_chk in levels:  # Use a different variable name to avoid clash
        level_key_chk = str(lev_val_chk)
        if (
            level_key_chk in comp_data
            and comp_data[level_key_chk].get("count") is not None
        ):
            count_final = comp_data[level_key_chk].get("count")
            logging.debug(
                f"  Final state L{lev_val_chk}: Count max = {np.max(count_final) if count_final.size > 0 else 'N/A'}"
            )
            for var_chk in ALL_VARS_TO_SAVE:
                sum_is_none = comp_data[level_key_chk].get(f"{var_chk}_sum") is None
                clim_sum_is_none = (
                    comp_data[level_key_chk].get(f"{var_chk}_clim_sum") is None
                )
                logging.debug(
                    f"    Final state L{lev_val_chk} V={var_chk}: sum is None? {sum_is_none}, clim_sum is None? {clim_sum_is_none}"
                )
        else:
            logging.debug(f"  Final state L{lev_val_chk}: No data or count is None.")

    return {"composite": comp_data}


# --- Combination Functions ---
def combine_tasks_results(
    task_results: List[Dict], levels: List[int]
) -> Dict[str, Any]:
    """Combine task results for composites."""
    logging.debug(f"--- Combining results for {len(task_results)} tasks ---")
    overall = {}
    for lev_val in levels:
        key = str(lev_val)
        overall[key] = {f"{var}_sum": None for var in ALL_VARS_TO_SAVE}
        overall[key].update({f"{var}_clim_sum": None for var in ALL_VARS_TO_SAVE})
        overall[key].update({"count": None, "lat": None, "lon": None})

    valid_task_count = 0
    for i, result_dict in enumerate(task_results):
        result = result_dict.get("composite", {})
        if not result:
            logging.debug(f"  Combine task {i+1}: Skipping (no 'composite' key)")
            continue

        task_had_valid_level = False
        for lev_val in levels:
            key = str(lev_val)
            res_level = result.get(key)
            if res_level is None or res_level.get("count") is None:
                continue

            count_val = res_level.get("count")
            if isinstance(count_val, (int, float)):
                count_val = np.array([[count_val]])
            if count_val.size == 0 or np.all(count_val == 0):
                continue

            task_had_valid_level = True
            logging.debug(
                f"  Combine task {i+1} L{lev_val}: Processing valid level. Count max: {np.max(count_val)}"
            )

            if overall[key]["lat"] is None and res_level.get("lat") is not None:
                overall[key]["lat"] = res_level.get("lat")
                overall[key]["lon"] = res_level.get("lon")
                overall[key]["count"] = count_val.astype(np.int32)
                for var in ALL_VARS_TO_SAVE:
                    overall[key][f"{var}_sum"] = (
                        res_level.get(f"{var}_sum")
                        if res_level.get(f"{var}_sum") is not None
                        else np.zeros_like(count_val, dtype=np.float64)
                    )
                    overall[key][f"{var}_clim_sum"] = (
                        res_level.get(f"{var}_clim_sum")
                        if res_level.get(f"{var}_clim_sum") is not None
                        else np.zeros_like(count_val, dtype=np.float64)
                    )
                logging.debug(f"    Initialized overall sums/counts for L{lev_val}.")
            elif overall[key]["lat"] is not None:
                if (
                    overall[key]["count"] is not None
                    and overall[key]["count"].shape == count_val.shape
                ):
                    overall[key]["count"] += count_val.astype(np.int32)
                    for var in ALL_VARS_TO_SAVE:
                        for sum_type_key_suffix in ["_sum", "_clim_sum"]:
                            sum_type = f"{var}{sum_type_key_suffix}"
                            current_sum = overall[key].get(sum_type)
                            new_sum = res_level.get(sum_type)
                            if new_sum is not None:
                                if (
                                    current_sum is not None
                                    and current_sum.shape == new_sum.shape
                                ):
                                    overall[key][sum_type] += new_sum
                                    logging.debug(
                                        f"      Accumulated {sum_type} L{lev_val}. New max: {np.nanmax(overall[key][sum_type]):.2f}"
                                    )
                                elif current_sum is None:
                                    overall[key][sum_type] = new_sum.copy()
                                    logging.debug(
                                        f"      Initialized {sum_type} L{lev_val} from task {i+1}."
                                    )
                                else:
                                    logging.warning(
                                        f"    Combine task {i+1} L{lev_val} Type={sum_type}: Shape mismatch. Overall: {current_sum.shape}, New: {new_sum.shape}. Skipping."
                                    )
                else:
                    logging.warning(
                        f"  Combine task {i+1} L{lev_val}: Shape mismatch for count. Overall: {overall[key]['count'].shape if overall[key]['count'] is not None else 'None'}, New: {count_val.shape}. Skipping."
                    )
        if task_had_valid_level:
            valid_task_count += 1

    logging.debug(
        f"--- Finished Combining Results. Processed {valid_task_count}/{len(task_results)} valid tasks. ---"
    )
    for lev_val in levels:
        key = str(lev_val)
        if overall[key].get("lat") is not None and overall[key].get("lon") is not None:
            final_shape = (len(overall[key]["lat"]), len(overall[key]["lon"]))
            for var in ALL_VARS_TO_SAVE:
                if overall[key].get(f"{var}_sum") is None:
                    logging.debug(
                        f"  Final combined {var}_sum L{lev_val} is None. Filling with NaN."
                    )
                    overall[key][f"{var}_sum"] = np.full(
                        final_shape, np.nan, dtype=np.float64
                    )
                if overall[key].get(f"{var}_clim_sum") is None:
                    logging.debug(
                        f"  Final combined {var}_clim_sum L{lev_val} is None. Filling with NaN."
                    )
                    overall[key][f"{var}_clim_sum"] = np.full(
                        final_shape, np.nan, dtype=np.float64
                    )
            if overall[key].get("count") is None:
                logging.debug(
                    f"  Final combined count L{lev_val} is None. Filling with zeros."
                )
                overall[key]["count"] = np.zeros(final_shape, dtype=np.int32)
        else:
            logging.warning(
                f"  No valid lat/lon found after combining tasks for L{lev_val}."
            )
    return overall


# --- NetCDF Saving Functions ---
def save_composites_to_netcdf(
    results_wt: Dict,
    weather_types: List,
    months: List[int],
    time_offsets: List[int],
    levels: List[int],
    lat: np.ndarray,
    lon: np.ndarray,
    output_file: Path,
    period_details: Dict,
    processed_years: Set[int],
    missing_years: List[int],
):
    """
    Save composite means (raw and climatology) and counts to a NetCDF file.
    Theta_e mean is now calculated like other variables (sum/count).
    Includes information about processed and missing years in attributes.
    """
    logging.info(f"--- Saving final composite NetCDF to: {output_file} ---")
    n_wt = len(weather_types)
    n_months = len(months)
    n_offsets = len(time_offsets)
    n_levels = len(levels)
    nlat = len(lat)
    nlon = len(lon)

    comp_arrays_mean = {
        var: np.full(
            (n_wt, n_months, n_offsets, n_levels, nlat, nlon), np.nan, dtype=np.float32
        )
        for var in ALL_VARS_TO_SAVE
    }
    comp_arrays_clim_mean = {
        var: np.full(
            (n_wt, n_months, n_offsets, n_levels, nlat, nlon), np.nan, dtype=np.float32
        )
        for var in ALL_VARS_TO_SAVE
    }
    comp_arrays_count_grid = np.full(
        (n_wt, n_months, n_offsets, n_levels, nlat, nlon), 0, dtype=np.int32
    )

    logging.debug("Populating final arrays for NetCDF...")
    for wi, wt in enumerate(weather_types):
        for mi, m_val in enumerate(months):
            for oi, off in enumerate(time_offsets):
                comp_month_data = results_wt.get(wt, {}).get(off, {}).get(m_val, None)
                if comp_month_data is None:
                    logging.debug(
                        f"  WT={wt}, M={m_val}, Off={off}h: No data for this group."
                    )
                    continue
                for li, lev_val in enumerate(levels):
                    key = str(lev_val)
                    group_label = f"WT={wt}, M={m_val}, Off={off}h, L={lev_val}hPa"
                    comp_level_data = comp_month_data.get(key)
                    if comp_level_data is None:
                        logging.debug(f"  {group_label}: No combined data for level.")
                        continue

                    count_arr = comp_level_data.get("count")
                    if (
                        count_arr is None
                        or count_arr.size == 0
                        or np.all(count_arr == 0)
                    ):
                        logging.debug(
                            f"  {group_label}: Combined count is zero or None."
                        )
                        continue

                    comp_arrays_count_grid[wi, mi, oi, li, :, :] = count_arr
                    logging.debug(f"  {group_label}: Count max = {np.max(count_arr)}")

                    # Calculate and store mean and climatology mean for ALL_VARS_TO_SAVE
                    for var in ALL_VARS_TO_SAVE:
                        for sum_type_suffix, mean_array_target in [
                            ("_sum", comp_arrays_mean[var]),
                            ("_clim_sum", comp_arrays_clim_mean[var]),
                        ]:
                            sum_type_key = f"{var}{sum_type_suffix}"
                            var_sum_val = comp_level_data.get(sum_type_key)
                            if var_sum_val is not None:
                                with np.errstate(
                                    divide="ignore", invalid="ignore"
                                ):  # Keep for division by zero
                                    mean_val = var_sum_val / count_arr
                                mean_array_target[wi, mi, oi, li, :, :] = np.where(
                                    count_arr > 0, mean_val, np.nan
                                )
                                if sum_type_suffix == "_sum":
                                    logging.debug(
                                        f"    {var}_mean calculated. Max: {np.nanmax(mean_val):.2f if np.any(np.isfinite(mean_val)) else 'NaN'}, NaN count: {np.isnan(mean_val).sum()}/{mean_val.size}"
                                    )
                                else:
                                    logging.debug(
                                        f"    {var}_clim_mean calculated. Max: {np.nanmax(mean_val):.2f if np.any(np.isfinite(mean_val)) else 'NaN'}, NaN count: {np.isnan(mean_val).sum()}/{mean_val.size}"
                                    )
                            else:
                                logging.debug(
                                    f"    {sum_type_key} was None for {group_label}."
                                )

    logging.debug("Creating xarray Dataset...")
    ds_vars = {}
    for var in ALL_VARS_TO_SAVE:
        ds_vars[f"{var}_mean"] = xr.DataArray(
            comp_arrays_mean[var],
            dims=(
                "weather_type",
                "month",
                "time_diff",
                "level",
                "latitude",
                "longitude",
            ),
            coords={
                "weather_type": weather_types,
                "month": months,
                "time_diff": time_offsets,
                "level": levels,
                "latitude": lat,
                "longitude": lon,
            },
            name=f"{var}_mean",
            attrs={"long_name": f"Mean raw {var}"},
        )
        ds_vars[f"{var}_clim_mean"] = xr.DataArray(
            comp_arrays_clim_mean[var],
            dims=(
                "weather_type",
                "month",
                "time_diff",
                "level",
                "latitude",
                "longitude",
            ),
            coords={
                "weather_type": weather_types,
                "month": months,
                "time_diff": time_offsets,
                "level": levels,
                "latitude": lat,
                "longitude": lon,
            },
            name=f"{var}_clim_mean",
            attrs={"long_name": f"Mean JJA climatology of {var}"},
        )
        if var == "theta_e":
            ds_vars[f"{var}_mean"].attrs["units"] = "K"
            ds_vars[f"{var}_clim_mean"].attrs["units"] = "K"
        # Placeholder for other variable units if known
        # elif var == 't': ds_vars[f"{var}_mean"].attrs['units'] = 'K'; ds_vars[f"{var}_clim_mean"].attrs['units'] = 'K'
        # ... etc.

    ds_vars["event_count"] = xr.DataArray(
        comp_arrays_count_grid,
        dims=("weather_type", "month", "time_diff", "level", "latitude", "longitude"),
        coords={
            "weather_type": weather_types,
            "month": months,
            "time_diff": time_offsets,
            "level": levels,
            "latitude": lat,
            "longitude": lon,
        },
        name="event_count",
        attrs={"long_name": "Number of events at each grid cell"},
    )

    ds = xr.Dataset(ds_vars)
    ds.attrs["description"] = (
        f"JJA ERA5 composites for pressure-level variables for MCS environments, "
        f"stratified by weather type. Theta_e calculated per timestep. Climatology based on JJA for period '{period_details['name_in_file']}' ({period_details['start']}-{period_details['end']})."
    )
    ds.attrs[
        "history"
    ] = f"Created by composite_pressure_levels.py on {pd.Timestamp.now(tz='UTC').strftime('%Y-%m-%d %H:%M:%S %Z')}"
    ds.attrs["source"] = "ERA5 pressure level data"
    ds.attrs["climatology_source_period_name"] = period_details["name_in_file"]
    ds.attrs[
        "climatology_source_period_years"
    ] = f"{period_details['start']}-{period_details['end']}"
    ds.attrs[
        "climatology_months_included"
    ] = "May-September (used for clim), composites for June-August"
    ds.attrs["data_period_years_processed"] = ", ".join(
        map(str, sorted(list(processed_years)))
    )
    ds.attrs["data_period_years_missing_in_csv"] = (
        ", ".join(map(str, sorted(missing_years))) if missing_years else "None"
    )

    encoding = {
        vname: {"zlib": True, "complevel": 4, "_FillValue": np.float32(1e20)}
        for vname in ds.data_vars
    }
    output_file.parent.mkdir(parents=True, exist_ok=True)
    ds.to_netcdf(output_file, encoding=encoding, mode="w")
    logging.info(f"Saved composite data to {output_file}")


# --- Main Script ---
def main():
    parser = argparse.ArgumentParser(
        description="Compute JJA monthly ERA5 pressure level composites for MCS events, using period-specific climatology. Theta_e calculated per timestep."
    )
    # ... (Argument parsing remains the same as original)
    parser.add_argument(
        "--era5_dir",
        type=Path,
        default="/data/reloclim/normal/INTERACT/ERA5/pressure_levels/",
        help="Directory containing ERA5 monthly files (e.g., 2005-08_NA.nc)",
    )
    parser.add_argument(
        "--clim_base_dir",
        type=Path,
        default="/home/dkn/climatology/ERA5/",
        help="Base directory where period-specific JJA climatology files are stored.",
    )
    parser.add_argument(
        "--period",
        type=str,
        default="evaluation",
        choices=PERIODS.keys(),
        help=f"Climatology period to use: {list(PERIODS.keys())}. Default: evaluation.",
    )
    parser.add_argument(
        "--comp_csv_base",
        type=str,
        default="/nas/home/dkn/Desktop/MoCCA/composites/scripts/synoptic_composites/csv/composite_",
        help="Base path for composite CSV files (e.g., './csv/composite_')",
    )
    parser.add_argument(
        "--levels",
        type=str,
        default="250,500,850",
        help="Comma-separated pressure levels in hPa (e.g., 250,500,850)",
    )
    parser.add_argument(
        "--region",
        type=str,
        required=True,
        help="Subregion name, used to find CSV file (e.g., southern_alps)",
    )
    parser.add_argument(
        "--output_dir",
        type=Path,
        default="/home/dkn/composites/ERA5/",
        help="Directory to save output composite netCDF files",
    )
    parser.add_argument(
        "--ncores", type=int, default=32, help="Number of cores for parallel processing"
    )
    parser.add_argument(
        "--serial", action="store_true", help="Run in serial mode for debugging"
    )
    parser.add_argument(
        "--time_offsets",
        type=str,
        default="-12,0,12",
        help="Comma-separated list of time offsets in hours (e.g., -12,-6,0,6,12)",
    )
    parser.add_argument(
        "--debug", action="store_true", help="Enable DEBUG level logging."
    )
    parser.add_argument(
        "--noMCS",
        action="store_true",
        help="False: composite of initMCS; True: composite of noMCS; Default=False",
    )
    args = parser.parse_args()

    log_level = logging.DEBUG if args.debug else logging.INFO
    log_filename = "composite_pressure_level.log"
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s",
        handlers=[
            logging.FileHandler(log_filename, mode="w"),
            logging.StreamHandler(sys.stdout),
        ],
        force=True,
    )
    logging.info(
        "--- Starting Pressure Level Composite Script (Theta_e per timestep) ---"
    )
    logging.info(f"Run arguments: {args}")
    logging.info(f"Logging output to: {log_filename}")

    levels_parsed = [int(l.strip()) for l in args.levels.split(",")]

    comp_csv_file_suffix = "_nomcs.csv" if args.noMCS else "_mcs.csv"
    comp_csv_file = Path(f"{args.comp_csv_base}{args.region}{comp_csv_file_suffix}")
    if not comp_csv_file.exists():
        logging.error(f"Composite CSV file not found: {comp_csv_file}")
        sys.exit(1)

    args_time_offsets_str = "0" if args.noMCS else args.time_offsets

    selected_period_details = PERIODS[args.period]
    clim_period_name_in_file = selected_period_details["name_in_file"]
    clim_start_year_period = selected_period_details["start"]
    clim_end_year_period = selected_period_details["end"]

    clim_files_to_load = []
    clim_base_var_dir = args.clim_base_dir
    # Ensure T and Q climatologies are loaded for theta_e_clim calculation.
    # VAR_LIST already contains 't' and 'q'.
    for var in VAR_LIST:
        fname = f"era5_plev_{var}_clim_may_sep_{clim_period_name_in_file}_{clim_start_year_period}-{clim_end_year_period}.nc"
        fpath = clim_base_var_dir / fname
        if not fpath.exists():
            logging.error(f"Required climatology file not found: {fpath}")
            logging.error(
                "Ensure climatologies for all VAR_LIST variables (esp. t, q for theta_e) exist for May-Sep."
            )
            sys.exit(1)
        clim_files_to_load.append(fpath)

    clim_ds = None
    datasets_to_merge = []
    # Removed try-except block for climatology loading
    logging.info(
        f"Loading and merging {len(clim_files_to_load)} climatology files for period '{args.period}'..."
    )
    for f in clim_files_to_load:
        logging.debug(f"  Opening clim file: {f}")
        datasets_to_merge.append(xr.open_dataset(f))

    clim_ds = xr.merge(
        datasets_to_merge, compat="override"
    )  # compat='override' can be risky, ensure coords are truly compatible or identical
    logging.info("Climatology files merged.")

    clim_ds = fix_lat_lon_names(clim_ds)
    clim_ds = clim_ds.sel(
        latitude=slice(DOMAIN_LAT_MIN, DOMAIN_LAT_MAX),
        longitude=slice(DOMAIN_LON_MIN, DOMAIN_LON_MAX),
    )
    clim_ds = reorder_lat(clim_ds)

    if not {"month", "hour", "level"}.issubset(clim_ds.dims):
        raise ValueError(
            "Merged climatology must have 'month', 'hour', 'level' dimensions."
        )
    missing_vars_in_clim = [
        v for v in VAR_LIST if v not in clim_ds
    ]  # Check for 't', 'q' specifically if used later
    if missing_vars_in_clim:
        raise ValueError(
            f"Base variables {missing_vars_in_clim} not found in merged climatology. 't' and 'q' are essential for theta_e_clim."
        )

    clim_months_present = np.sort(clim_ds.month.values)
    if not np.array_equal(clim_months_present, CLIMATOLOGY_MONTHS):
        logging.warning(
            f"Merged climatology months {clim_months_present} do not match expected May-Sep {CLIMATOLOGY_MONTHS}."
        )
    logging.info(
        f"Climatology for PLEV vars (Period: {args.period}, Months: May-Sep) loaded and merged."
    )

    # Close individual datasets after merging (moved out of try-finally)
    for ds_item in datasets_to_merge:
        ds_item.close()

    base_col = "datetime" if args.noMCS else "time_0h"
    # Removed try-except for CSV loading
    df_all = pd.read_csv(
        comp_csv_file, parse_dates=[base_col]
    )  # parse_dates will raise if col not found or unparsable
    df_all[base_col] = df_all[base_col].dt.round("H")

    df_all["year"] = df_all[base_col].dt.year
    df_all["event_month_for_filter"] = df_all[base_col].dt.month

    logging.info(
        f"Filtering events for period: {args.period} ({clim_start_year_period}-{clim_end_year_period})"
    )
    df_period = df_all[
        (df_all["year"] >= clim_start_year_period)
        & (df_all["year"] <= clim_end_year_period)
    ].copy()

    target_years_set = set(range(clim_start_year_period, clim_end_year_period + 1))
    processed_years_set = set(df_period["year"].unique())
    missing_years_list = sorted(list(target_years_set - processed_years_set))

    if not processed_years_set:
        logging.warning(
            f"No events found in the CSV for the selected period {args.period}. Exiting."
        )
        if clim_ds is not None:
            clim_ds.close()
        sys.exit(0)
    # ... (logging for processed/missing years remains)
    logging.info(
        f"Years processed from CSV within period: {sorted(list(processed_years_set))}"
    )
    if missing_years_list:
        logging.warning(
            f"Years missing in CSV within period {args.period}: {missing_years_list}"
        )
    else:
        logging.info(f"All years within period {args.period} present in CSV.")

    logging.info(f"Filtering events for target months (JJA): {TARGET_MONTHS}")
    df_filtered = df_period[
        df_period["event_month_for_filter"].isin(TARGET_MONTHS)
    ].copy()

    if df_filtered.empty:
        logging.info(
            f"No events found in JJA for region {args.region} within period {args.period}. Exiting."
        )
        if clim_ds is not None:
            clim_ds.close()
        sys.exit(0)

    # Removed try-except for offset columns
    offset_col_names = create_offset_cols(
        df_filtered
    )  # Will raise ValueError if issues
    time_offsets_parsed = sorted(
        [int(o) for o in args_time_offsets_str.split(",")]
    )  # Will raise ValueError if not int

    missing_offsets = [
        off for off in time_offsets_parsed if off not in offset_col_names
    ]
    if missing_offsets:
        # This is a fatal configuration error.
        raise ValueError(
            f"Requested time offsets {missing_offsets} not found in CSV columns: {list(offset_col_names.values())}"
        )

    months_to_process_for_output = sorted(
        df_filtered["event_month_for_filter"].unique()
    )
    logging.info(
        f"Processing events for months: {months_to_process_for_output} for region {args.region}"
    )

    if "wt" not in df_filtered.columns:
        raise KeyError(f"Weather type column 'wt' not found in {comp_csv_file}.")
    weather_types_from_csv = sorted(list(df_filtered["wt"].unique()))
    weather_types_to_process = (
        [0] + weather_types_from_csv
        if 0 not in weather_types_from_csv
        else weather_types_from_csv
    )
    logging.info(f"Processing weather types: {weather_types_to_process}")

    results_wt = {
        wt: {off: {} for off in time_offsets_parsed} for wt in weather_types_to_process
    }

    logging.info("--- Starting Processing Loops ---")
    for wt_val in weather_types_to_process:  # Use wt_val to avoid clash
        df_wt = (
            df_filtered[df_filtered["wt"] == wt_val].copy()
            if wt_val != 0
            else df_filtered.copy()
        )
        logging.info(
            f"Processing WT {wt_val} ('{'All Events' if wt_val==0 else f'Type {wt_val}'}') - {len(df_wt)} events (JJA, Period: {args.period})"
        )
        if df_wt.empty:
            continue

        for off_val in time_offsets_parsed:  # Use off_val
            offset_col = offset_col_names[off_val]
            # This check should not be strictly necessary if missing_offsets check passed, but good for safety.
            if offset_col not in df_wt.columns:
                logging.error(
                    f"Offset column '{offset_col}' missing for WT {wt_val}, offset {off_val}h. This should not happen."
                )
                # This implies a logic error earlier, should crash or be handled as fatal.
                # For now, continue, but this indicates an issue if reached.
                continue

            logging.info(f"  Processing Offset: {off_val}h (Column: {offset_col})")
            df_wt_off = df_wt.copy()  # Work on a copy for this offset
            if not pd.api.types.is_datetime64_any_dtype(df_wt_off[offset_col]):
                df_wt_off[offset_col] = pd.to_datetime(
                    df_wt_off[offset_col], errors="coerce"
                )
            df_wt_off = df_wt_off.dropna(
                subset=[offset_col]
            )  # Critical: remove rows where time conversion failed

            df_wt_off["year_for_grouping"] = df_wt_off[offset_col].dt.year
            df_wt_off["month_for_grouping"] = df_wt_off[
                "event_month_for_filter"
            ]  # Use original JJA event month
            groups = df_wt_off.groupby(["year_for_grouping", "month_for_grouping"])

            tasks = []
            for (year_group, month_group), group_data in groups:
                t_list_pd = pd.DatetimeIndex(group_data[offset_col].tolist())
                if not t_list_pd.empty:
                    tasks.append(
                        (
                            year_group,
                            month_group,
                            t_list_pd,
                            args.era5_dir,
                            levels_parsed,
                            clim_ds,
                        )
                    )

            logging.info(
                f"    WT {wt_val}, Offset {off_val}h: Created {len(tasks)} tasks (year-month groups)."
            )
            if not tasks:
                continue

            monthly_tasks_results = []
            if args.serial or args.ncores == 1:
                logging.info(f"    Running {len(tasks)} tasks serially...")
                for task_idx, task_data in enumerate(tasks):
                    logging.debug(
                        f"      Serial task {task_idx+1}/{len(tasks)}: Year {task_data[0]}, Month {task_data[1]}"
                    )
                    monthly_tasks_results.append(process_month_level_events(task_data))
            else:
                logging.info(
                    f"    Running {len(tasks)} tasks in parallel using {args.ncores} cores..."
                )
                with Pool(processes=args.ncores) as pool:
                    monthly_tasks_results = pool.map(process_month_level_events, tasks)
            logging.info(
                f"    Finished processing tasks for WT {wt_val}, Offset {off_val}h."
            )

            tasks_by_month_comp = {}
            for task_data, res in zip(tasks, monthly_tasks_results):
                (
                    _,
                    month_val_task,
                    _,
                    _,
                    _,
                    _,
                ) = task_data  # month_val_task is the month_for_grouping
                tasks_by_month_comp.setdefault(month_val_task, []).append(res)

            month_composites = {}
            logging.info(f"    Combining results for WT {wt_val}, Offset {off_val}h...")
            for m_out in months_to_process_for_output:  # Iterate JJA months
                if m_out in tasks_by_month_comp:
                    month_composites[m_out] = combine_tasks_results(
                        tasks_by_month_comp[m_out], levels_parsed
                    )
                else:
                    # Initialize empty structure for this output month if no tasks contributed
                    month_composites[m_out] = {
                        str(lvl): {
                            **{f"{var}_sum": None for var in ALL_VARS_TO_SAVE},
                            **{f"{var}_clim_sum": None for var in ALL_VARS_TO_SAVE},
                            "count": None,
                            "lat": None,
                            "lon": None,
                        }
                        for lvl in levels_parsed
                    }
            results_wt[wt_val][off_val] = month_composites
            logging.info(f"    Finished combining for WT {wt_val}, Offset {off_val}h.")
    logging.info("--- Finished Processing Loops ---")

    logging.info("--- Post-processing and Saving Results ---")
    lat_coords, lon_coords = None, None  # Renamed to avoid clash
    # Removed try-except for lat/lon retrieval
    found_coords = False
    for wt_chk in weather_types_to_process:
        for off_chk in time_offsets_parsed:
            for m_chk in months_to_process_for_output:
                for lev_chk in levels_parsed:
                    sample_comp = (
                        results_wt.get(wt_chk, {})
                        .get(off_chk, {})
                        .get(m_chk, {})
                        .get(str(lev_chk), None)
                    )
                    if (
                        sample_comp
                        and sample_comp.get("lat") is not None
                        and sample_comp.get("lon") is not None
                    ):
                        lat_coords = sample_comp["lat"]
                        lon_coords = sample_comp["lon"]
                        found_coords = True
                        break
                if found_coords:
                    break
            if found_coords:
                break
        if found_coords:
            break

    if not found_coords:
        logging.warning(
            "Could not find valid lat/lon in any processed composite. Trying clim_ds for coords."
        )
        if clim_ds is not None and "latitude" in clim_ds and "longitude" in clim_ds:
            lat_coords = clim_ds["latitude"].values
            lon_coords = clim_ds["longitude"].values
            if lat_coords is not None and lon_coords is not None:
                found_coords = True
        if not found_coords:
            raise ValueError(
                "Could not find valid lat/lon coordinates from results or climatology. Cannot save."
            )

    if lat_coords is None or lon_coords is None:  # Should be caught by the above raise
        raise ValueError(
            "Lat/lon coordinate information is missing. Cannot save output file."
        )

    output_file_name_suffix = "_nomcs.nc" if args.noMCS else ".nc"
    output_file_comp = (
        args.output_dir
        / f"composite_plev_{args.region}_wt_clim_{args.period}{output_file_name_suffix}"
    )

    save_composites_to_netcdf(
        results_wt,
        weather_types_to_process,
        months_to_process_for_output,
        time_offsets_parsed,
        levels_parsed,
        lat_coords,
        lon_coords,
        output_file_comp,
        selected_period_details,
        processed_years=processed_years_set,
        missing_years=missing_years_list,
    )

    if clim_ds is not None:
        clim_ds.close()
    logging.info("--- Processing complete. ---")


if __name__ == "__main__":
    warnings.filterwarnings(
        "ignore", message="Relative humidity >120%, ensure proper units."
    )  # From MetPy
    warnings.filterwarnings(
        "ignore", message="invalid value encountered in divide"
    )  # For mean calculation if count is 0 (though handled by np.where)
    warnings.filterwarnings(
        "ignore", message="Mean of empty slice"
    )  # Can occur in numpy operations
    warnings.filterwarnings(
        "ignore", message=".*Slicing is producing a large chunk.*"
    )  # Xarray performance hint
    warnings.filterwarnings(
        "ignore",
        message=".*Converting non-nanosecond precision datetime values to nanosecond precision.*",
    )  # Pandas/xarray
    main()
